maxpool
latent size single: 16
loading dataset
16
pccppcpc1123 (16, 27, 48, 3)
creating model
start training
[8/15000], training loss: 0.1919
[16/15000], training loss: 0.1455
[24/15000], training loss: 0.1364
[32/15000], training loss: 0.1344
[40/15000], training loss: 0.1268
16
AVD_Home_010_1_traj8, ate: 303.27463752882585
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[48/15000], training loss: 0.1276
[56/15000], training loss: 0.1285
[64/15000], training loss: 0.1267
[72/15000], training loss: 0.1287
[80/15000], training loss: 0.1262
16
AVD_Home_010_1_traj8, ate: 296.2098200805736
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[88/15000], training loss: 0.1279
[96/15000], training loss: 0.1199
[104/15000], training loss: 0.1133
[112/15000], training loss: 0.1322
[120/15000], training loss: 0.1263
16
AVD_Home_010_1_traj8, ate: 302.07414789711834
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[128/15000], training loss: 0.1255
[136/15000], training loss: 0.1156
[144/15000], training loss: 0.1237
[152/15000], training loss: 0.1281
[160/15000], training loss: 0.1154
16
AVD_Home_010_1_traj8, ate: 388.0062024871405
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[168/15000], training loss: 0.1227
[176/15000], training loss: 0.1161
[184/15000], training loss: 0.1239
[192/15000], training loss: 0.1217
[200/15000], training loss: 0.1143
16
AVD_Home_010_1_traj8, ate: 392.7489166351269
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[208/15000], training loss: 0.1227
[216/15000], training loss: 0.1156
[224/15000], training loss: 0.1258
[232/15000], training loss: 0.1207
[240/15000], training loss: 0.1184
16
AVD_Home_010_1_traj8, ate: 357.0833825278336
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[248/15000], training loss: 0.1163
[256/15000], training loss: 0.1184
[264/15000], training loss: 0.1188
[272/15000], training loss: 0.1211
[280/15000], training loss: 0.1099
16
AVD_Home_010_1_traj8, ate: 403.7710170322807
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[288/15000], training loss: 0.1195
[296/15000], training loss: 0.1192
[304/15000], training loss: 0.1176
[312/15000], training loss: 0.1226
[320/15000], training loss: 0.1124
16
AVD_Home_010_1_traj8, ate: 389.69678345685287
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[328/15000], training loss: 0.1199
[336/15000], training loss: 0.1105
[344/15000], training loss: 0.1046
[352/15000], training loss: 0.1076
[360/15000], training loss: 0.1141
16
AVD_Home_010_1_traj8, ate: 409.6264540743183
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[368/15000], training loss: 0.1218
[376/15000], training loss: 0.1156
[384/15000], training loss: 0.1131
[392/15000], training loss: 0.1210
[400/15000], training loss: 0.1143
16
AVD_Home_010_1_traj8, ate: 473.577824107187
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[408/15000], training loss: 0.1112
[416/15000], training loss: 0.1218
[424/15000], training loss: 0.1228
[432/15000], training loss: 0.1177
[440/15000], training loss: 0.1141
16
AVD_Home_010_1_traj8, ate: 443.2417585301902
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[448/15000], training loss: 0.1180
[456/15000], training loss: 0.1160
[464/15000], training loss: 0.1059
[472/15000], training loss: 0.1122
[480/15000], training loss: 0.1164
16
AVD_Home_010_1_traj8, ate: 498.87861302965615
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[488/15000], training loss: 0.1190
[496/15000], training loss: 0.1182
[504/15000], training loss: 0.1092
[512/15000], training loss: 0.1185
[520/15000], training loss: 0.1231
16
AVD_Home_010_1_traj8, ate: 422.65259596661446
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[528/15000], training loss: 0.1201
[536/15000], training loss: 0.1031
[544/15000], training loss: 0.1093
[552/15000], training loss: 0.1300
[560/15000], training loss: 0.1170
16
AVD_Home_010_1_traj8, ate: 317.3767488745606
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[568/15000], training loss: 0.1162
[576/15000], training loss: 0.1113
[584/15000], training loss: 0.1136
[592/15000], training loss: 0.1126
[600/15000], training loss: 0.1147
16
AVD_Home_010_1_traj8, ate: 323.4682365868795
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[608/15000], training loss: 0.1087
[616/15000], training loss: 0.0962
[624/15000], training loss: 0.1284
[632/15000], training loss: 0.1002
[640/15000], training loss: 0.1075
16
AVD_Home_010_1_traj8, ate: 299.40966607707117
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[648/15000], training loss: 0.0980
[656/15000], training loss: 0.1018
[664/15000], training loss: 0.1082
[672/15000], training loss: 0.1124
[680/15000], training loss: 0.1139
16
AVD_Home_010_1_traj8, ate: 289.6301280227416
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[688/15000], training loss: 0.1103
[696/15000], training loss: 0.1016
[704/15000], training loss: 0.1049
[712/15000], training loss: 0.1126
[720/15000], training loss: 0.0982
16
AVD_Home_010_1_traj8, ate: 259.43651197636444
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[728/15000], training loss: 0.1132
[736/15000], training loss: 0.1110
[744/15000], training loss: 0.1141
[752/15000], training loss: 0.1077
[760/15000], training loss: 0.0998
16
AVD_Home_010_1_traj8, ate: 244.51807246962937
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[768/15000], training loss: 0.1050
[776/15000], training loss: 0.1110
[784/15000], training loss: 0.1102
[792/15000], training loss: 0.1102
[800/15000], training loss: 0.1061
16
AVD_Home_010_1_traj8, ate: 221.60334392767723
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[808/15000], training loss: 0.0988
[816/15000], training loss: 0.1054
[824/15000], training loss: 0.1062
[832/15000], training loss: 0.1022
[840/15000], training loss: 0.0942
16
AVD_Home_010_1_traj8, ate: 217.73657200472516
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[848/15000], training loss: 0.1057
[856/15000], training loss: 0.1034
[864/15000], training loss: 0.1275
[872/15000], training loss: 0.1140
[880/15000], training loss: 0.1115
16
AVD_Home_010_1_traj8, ate: 221.0450013791872
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[888/15000], training loss: 0.1095
[896/15000], training loss: 0.1007
[904/15000], training loss: 0.0916
[912/15000], training loss: 0.0966
[920/15000], training loss: 0.1057
16
AVD_Home_010_1_traj8, ate: 256.7494578521949
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[928/15000], training loss: 0.0906
[936/15000], training loss: 0.0997
[944/15000], training loss: 0.1040
[952/15000], training loss: 0.0920
[960/15000], training loss: 0.0899
16
AVD_Home_010_1_traj8, ate: 246.42123978531725
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[968/15000], training loss: 0.1052
[976/15000], training loss: 0.0914
[984/15000], training loss: 0.1025
[992/15000], training loss: 0.1123
[1000/15000], training loss: 0.0982
16
AVD_Home_010_1_traj8, ate: 248.29992456743963
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[1008/15000], training loss: 0.1039
[1016/15000], training loss: 0.0945
[1024/15000], training loss: 0.0901
[1032/15000], training loss: 0.0934
[1040/15000], training loss: 0.1013
16
AVD_Home_010_1_traj8, ate: 241.82069384945345
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[1048/15000], training loss: 0.1015
[1056/15000], training loss: 0.1020
[1064/15000], training loss: 0.1058
[1072/15000], training loss: 0.0928
[1080/15000], training loss: 0.1023
16
AVD_Home_010_1_traj8, ate: 248.81290411539067
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[1088/15000], training loss: 0.1057
[1096/15000], training loss: 0.0955
[1104/15000], training loss: 0.0986
[1112/15000], training loss: 0.0920
[1120/15000], training loss: 0.0892
16
AVD_Home_010_1_traj8, ate: 264.6015003638461
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[1128/15000], training loss: 0.1169
[1136/15000], training loss: 0.1064
[1144/15000], training loss: 0.1056
[1152/15000], training loss: 0.1078
[1160/15000], training loss: 0.1095
16
AVD_Home_010_1_traj8, ate: 229.41652129506826
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[1168/15000], training loss: 0.0895
[1176/15000], training loss: 0.0995
[1184/15000], training loss: 0.1006
[1192/15000], training loss: 0.0964
[1200/15000], training loss: 0.0918
16
AVD_Home_010_1_traj8, ate: 264.8073861595949
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[1208/15000], training loss: 0.0926
[1216/15000], training loss: 0.0916
[1224/15000], training loss: 0.0964
[1232/15000], training loss: 0.0841
[1240/15000], training loss: 0.0920
16
AVD_Home_010_1_traj8, ate: 267.5498496255841
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[1248/15000], training loss: 0.1066
[1256/15000], training loss: 0.0890
[1264/15000], training loss: 0.0968
[1272/15000], training loss: 0.0915
[1280/15000], training loss: 0.0877
16
AVD_Home_010_1_traj8, ate: 262.4040010585068
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[1288/15000], training loss: 0.0886
[1296/15000], training loss: 0.0997
[1304/15000], training loss: 0.0895
[1312/15000], training loss: 0.0968
[1320/15000], training loss: 0.0982
16
AVD_Home_010_1_traj8, ate: 272.7416814945048
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[1328/15000], training loss: 0.0798
[1336/15000], training loss: 0.1021
[1344/15000], training loss: 0.0857
[1352/15000], training loss: 0.0992
[1360/15000], training loss: 0.0834
16
AVD_Home_010_1_traj8, ate: 253.9873916696229
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[1368/15000], training loss: 0.0936
[1376/15000], training loss: 0.0929
[1384/15000], training loss: 0.0874
[1392/15000], training loss: 0.0922
[1400/15000], training loss: 0.1163
16
AVD_Home_010_1_traj8, ate: 267.0754470827075
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[1408/15000], training loss: 0.0958
[1416/15000], training loss: 0.0838
[1424/15000], training loss: 0.1027
[1432/15000], training loss: 0.0991
[1440/15000], training loss: 0.0962
16
AVD_Home_010_1_traj8, ate: 262.98739623977775
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[1448/15000], training loss: 0.0959
[1456/15000], training loss: 0.0858
[1464/15000], training loss: 0.0866
[1472/15000], training loss: 0.1051
[1480/15000], training loss: 0.0900
16
AVD_Home_010_1_traj8, ate: 248.11589638954482
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[1488/15000], training loss: 0.0929
[1496/15000], training loss: 0.0836
[1504/15000], training loss: 0.0913
[1512/15000], training loss: 0.0860
[1520/15000], training loss: 0.0837
16
AVD_Home_010_1_traj8, ate: 265.8493314702403
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[1528/15000], training loss: 0.1035
[1536/15000], training loss: 0.0955
[1544/15000], training loss: 0.0948
[1552/15000], training loss: 0.0936
[1560/15000], training loss: 0.0843
16
AVD_Home_010_1_traj8, ate: 264.51415445047206
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[1568/15000], training loss: 0.0978
[1576/15000], training loss: 0.1068
[1584/15000], training loss: 0.0811
[1592/15000], training loss: 0.0827
[1600/15000], training loss: 0.0988
16
AVD_Home_010_1_traj8, ate: 252.54800919050092
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[1608/15000], training loss: 0.0869
[1616/15000], training loss: 0.0831
[1624/15000], training loss: 0.0857
[1632/15000], training loss: 0.0972
[1640/15000], training loss: 0.0817
16
AVD_Home_010_1_traj8, ate: 240.00573372333025
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[1648/15000], training loss: 0.0969
[1656/15000], training loss: 0.1054
[1664/15000], training loss: 0.1018
[1672/15000], training loss: 0.0891
[1680/15000], training loss: 0.0819
16
AVD_Home_010_1_traj8, ate: 254.49455274799854
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[1688/15000], training loss: 0.0882
[1696/15000], training loss: 0.0823
[1704/15000], training loss: 0.0969
[1712/15000], training loss: 0.0973
[1720/15000], training loss: 0.0987
16
AVD_Home_010_1_traj8, ate: 236.77125957292455
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[1728/15000], training loss: 0.0831
[1736/15000], training loss: 0.0765
[1744/15000], training loss: 0.0838
[1752/15000], training loss: 0.1002
[1760/15000], training loss: 0.0814
16
AVD_Home_010_1_traj8, ate: 254.26024356208367
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[1768/15000], training loss: 0.0983
[1776/15000], training loss: 0.0931
[1784/15000], training loss: 0.0864
[1792/15000], training loss: 0.0856
[1800/15000], training loss: 0.0993
16
AVD_Home_010_1_traj8, ate: 254.64593621681752
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[1808/15000], training loss: 0.0930
[1816/15000], training loss: 0.0933
[1824/15000], training loss: 0.1050
[1832/15000], training loss: 0.0952
[1840/15000], training loss: 0.1003
16
AVD_Home_010_1_traj8, ate: 257.4557628599484
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[1848/15000], training loss: 0.0898
[1856/15000], training loss: 0.0868
[1864/15000], training loss: 0.0693
[1872/15000], training loss: 0.0909
[1880/15000], training loss: 0.0893
16
AVD_Home_010_1_traj8, ate: 238.97495496117708
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[1888/15000], training loss: 0.0959
[1896/15000], training loss: 0.0935
[1904/15000], training loss: 0.0916
[1912/15000], training loss: 0.0888
[1920/15000], training loss: 0.0962
16
AVD_Home_010_1_traj8, ate: 232.56460030594727
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[1928/15000], training loss: 0.0906
[1936/15000], training loss: 0.0996
[1944/15000], training loss: 0.0983
[1952/15000], training loss: 0.0873
[1960/15000], training loss: 0.0845
16
AVD_Home_010_1_traj8, ate: 230.0345806691716
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[1968/15000], training loss: 0.1224
[1976/15000], training loss: 0.1027
[1984/15000], training loss: 0.0936
[1992/15000], training loss: 0.1029
[2000/15000], training loss: 0.0815
16
AVD_Home_010_1_traj8, ate: 241.87074704517826
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[2008/15000], training loss: 0.0856
[2016/15000], training loss: 0.1036
[2024/15000], training loss: 0.0883
[2032/15000], training loss: 0.0811
[2040/15000], training loss: 0.1082
16
AVD_Home_010_1_traj8, ate: 239.53607498464828
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[2048/15000], training loss: 0.0867
[2056/15000], training loss: 0.0777
[2064/15000], training loss: 0.0955
[2072/15000], training loss: 0.1012
[2080/15000], training loss: 0.1086
16
AVD_Home_010_1_traj8, ate: 233.22545375447004
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[2088/15000], training loss: 0.1220
[2096/15000], training loss: 0.0989
[2104/15000], training loss: 0.0869
[2112/15000], training loss: 0.0738
[2120/15000], training loss: 0.0979
16
AVD_Home_010_1_traj8, ate: 241.6919588828734
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[2128/15000], training loss: 0.0948
[2136/15000], training loss: 0.0906
[2144/15000], training loss: 0.0896
[2152/15000], training loss: 0.0963
[2160/15000], training loss: 0.0928
16
AVD_Home_010_1_traj8, ate: 227.0590894605993
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[2168/15000], training loss: 0.0714
[2176/15000], training loss: 0.0806
[2184/15000], training loss: 0.0862
[2192/15000], training loss: 0.0821
[2200/15000], training loss: 0.0822
16
AVD_Home_010_1_traj8, ate: 231.1251640316666
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[2208/15000], training loss: 0.0642
[2216/15000], training loss: 0.0799
[2224/15000], training loss: 0.0665
[2232/15000], training loss: 0.0868
[2240/15000], training loss: 0.0805
16
AVD_Home_010_1_traj8, ate: 220.18221799665122
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[2248/15000], training loss: 0.0783
[2256/15000], training loss: 0.0906
[2264/15000], training loss: 0.0966
[2272/15000], training loss: 0.0735
[2280/15000], training loss: 0.0768
16
AVD_Home_010_1_traj8, ate: 219.43514841520937
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[2288/15000], training loss: 0.0791
[2296/15000], training loss: 0.1360
[2304/15000], training loss: 0.0919
[2312/15000], training loss: 0.0738
[2320/15000], training loss: 0.0827
16
AVD_Home_010_1_traj8, ate: 225.8688215836298
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[2328/15000], training loss: 0.0868
[2336/15000], training loss: 0.0868
[2344/15000], training loss: 0.0776
[2352/15000], training loss: 0.0808
[2360/15000], training loss: 0.0924
16
AVD_Home_010_1_traj8, ate: 231.66884942109914
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[2368/15000], training loss: 0.0666
[2376/15000], training loss: 0.0900
[2384/15000], training loss: 0.1056
[2392/15000], training loss: 0.0844
[2400/15000], training loss: 0.0812
16
AVD_Home_010_1_traj8, ate: 233.84859894133686
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[2408/15000], training loss: 0.0797
[2416/15000], training loss: 0.0746
[2424/15000], training loss: 0.0805
[2432/15000], training loss: 0.0830
[2440/15000], training loss: 0.0966
16
AVD_Home_010_1_traj8, ate: 213.47476623330104
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[2448/15000], training loss: 0.0935
[2456/15000], training loss: 0.0853
[2464/15000], training loss: 0.0938
[2472/15000], training loss: 0.0720
[2480/15000], training loss: 0.0783
16
AVD_Home_010_1_traj8, ate: 222.5729940230307
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[2488/15000], training loss: 0.0852
[2496/15000], training loss: 0.0844
[2504/15000], training loss: 0.0922
[2512/15000], training loss: 0.0872
[2520/15000], training loss: 0.0805
16
AVD_Home_010_1_traj8, ate: 225.81170072361883
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[2528/15000], training loss: 0.0862
[2536/15000], training loss: 0.0750
[2544/15000], training loss: 0.0737
[2552/15000], training loss: 0.0790
[2560/15000], training loss: 0.0719
16
AVD_Home_010_1_traj8, ate: 216.42004289554953
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[2568/15000], training loss: 0.0911
[2576/15000], training loss: 0.0707
[2584/15000], training loss: 0.0866
[2592/15000], training loss: 0.0843
[2600/15000], training loss: 0.0925
16
AVD_Home_010_1_traj8, ate: 214.20136792864662
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[2608/15000], training loss: 0.0927
[2616/15000], training loss: 0.0818
[2624/15000], training loss: 0.0803
[2632/15000], training loss: 0.0851
[2640/15000], training loss: 0.0905
16
AVD_Home_010_1_traj8, ate: 226.48323494063627
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[2648/15000], training loss: 0.0732
[2656/15000], training loss: 0.0930
[2664/15000], training loss: 0.0897
[2672/15000], training loss: 0.0843
[2680/15000], training loss: 0.1010
16
AVD_Home_010_1_traj8, ate: 223.972881239946
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[2688/15000], training loss: 0.0977
[2696/15000], training loss: 0.0859
[2704/15000], training loss: 0.0696
[2712/15000], training loss: 0.0960
[2720/15000], training loss: 0.1161
16
AVD_Home_010_1_traj8, ate: 222.630004440945
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[2728/15000], training loss: 0.0743
[2736/15000], training loss: 0.0809
[2744/15000], training loss: 0.0708
[2752/15000], training loss: 0.0982
[2760/15000], training loss: 0.0871
16
AVD_Home_010_1_traj8, ate: 227.8233381591396
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[2768/15000], training loss: 0.0845
[2776/15000], training loss: 0.0813
[2784/15000], training loss: 0.0941
[2792/15000], training loss: 0.0876
[2800/15000], training loss: 0.0664
16
AVD_Home_010_1_traj8, ate: 228.6688223210414
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[2808/15000], training loss: 0.0792
[2816/15000], training loss: 0.0615
[2824/15000], training loss: 0.0687
[2832/15000], training loss: 0.0785
[2840/15000], training loss: 0.0858
16
AVD_Home_010_1_traj8, ate: 212.31451782977706
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[2848/15000], training loss: 0.0712
[2856/15000], training loss: 0.0674
[2864/15000], training loss: 0.0950
[2872/15000], training loss: 0.0849
[2880/15000], training loss: 0.0827
16
AVD_Home_010_1_traj8, ate: 219.65421554418361
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[2888/15000], training loss: 0.0767
[2896/15000], training loss: 0.0951
[2904/15000], training loss: 0.0707
[2912/15000], training loss: 0.0761
[2920/15000], training loss: 0.1028
16
AVD_Home_010_1_traj8, ate: 214.08132240628524
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[2928/15000], training loss: 0.0821
[2936/15000], training loss: 0.0709
[2944/15000], training loss: 0.0828
[2952/15000], training loss: 0.0772
[2960/15000], training loss: 0.0817
16
AVD_Home_010_1_traj8, ate: 208.99164563076286
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[2968/15000], training loss: 0.0825
[2976/15000], training loss: 0.0938
[2984/15000], training loss: 0.0706
[2992/15000], training loss: 0.0813
[3000/15000], training loss: 0.0728
16
AVD_Home_010_1_traj8, ate: 218.24664207977162
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[3008/15000], training loss: 0.0679
[3016/15000], training loss: 0.0650
[3024/15000], training loss: 0.0671
[3032/15000], training loss: 0.0635
[3040/15000], training loss: 0.0807
16
AVD_Home_010_1_traj8, ate: 214.5820261318205
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[3048/15000], training loss: 0.0630
[3056/15000], training loss: 0.0663
[3064/15000], training loss: 0.0808
[3072/15000], training loss: 0.0840
[3080/15000], training loss: 0.0945
16
AVD_Home_010_1_traj8, ate: 211.85495741559635
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[3088/15000], training loss: 0.1080
[3096/15000], training loss: 0.0715
[3104/15000], training loss: 0.0766
[3112/15000], training loss: 0.0728
[3120/15000], training loss: 0.0728
16
AVD_Home_010_1_traj8, ate: 214.61714280834093
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[3128/15000], training loss: 0.0660
[3136/15000], training loss: 0.0573
[3144/15000], training loss: 0.0767
[3152/15000], training loss: 0.0848
[3160/15000], training loss: 0.0763
16
AVD_Home_010_1_traj8, ate: 210.08746976773497
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[3168/15000], training loss: 0.0678
[3176/15000], training loss: 0.0573
[3184/15000], training loss: 0.1110
[3192/15000], training loss: 0.0921
[3200/15000], training loss: 0.0718
16
AVD_Home_010_1_traj8, ate: 215.22524596153335
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[3208/15000], training loss: 0.0669
[3216/15000], training loss: 0.1096
[3224/15000], training loss: 0.0966
[3232/15000], training loss: 0.0865
[3240/15000], training loss: 0.0768
16
AVD_Home_010_1_traj8, ate: 227.8134707045454
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[3248/15000], training loss: 0.0690
[3256/15000], training loss: 0.0669
[3264/15000], training loss: 0.0826
[3272/15000], training loss: 0.0800
[3280/15000], training loss: 0.0571
16
AVD_Home_010_1_traj8, ate: 213.4188079656908
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[3288/15000], training loss: 0.0640
[3296/15000], training loss: 0.0804
[3304/15000], training loss: 0.0696
[3312/15000], training loss: 0.0736
[3320/15000], training loss: 0.0681
16
AVD_Home_010_1_traj8, ate: 213.71477161349688
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[3328/15000], training loss: 0.0861
[3336/15000], training loss: 0.0673
[3344/15000], training loss: 0.0874
[3352/15000], training loss: 0.0767
[3360/15000], training loss: 0.0863
16
AVD_Home_010_1_traj8, ate: 212.94595861151313
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[3368/15000], training loss: 0.0837
[3376/15000], training loss: 0.0645
[3384/15000], training loss: 0.0785
[3392/15000], training loss: 0.0923
[3400/15000], training loss: 0.0645
16
AVD_Home_010_1_traj8, ate: 207.10425687586869
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[3408/15000], training loss: 0.0701
[3416/15000], training loss: 0.0758
[3424/15000], training loss: 0.0752
[3432/15000], training loss: 0.0838
[3440/15000], training loss: 0.0810
16
AVD_Home_010_1_traj8, ate: 213.57825825432988
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[3448/15000], training loss: 0.0881
[3456/15000], training loss: 0.1064
[3464/15000], training loss: 0.0767
[3472/15000], training loss: 0.0898
[3480/15000], training loss: 0.0717
16
AVD_Home_010_1_traj8, ate: 208.7569968008569
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[3488/15000], training loss: 0.0771
[3496/15000], training loss: 0.0966
[3504/15000], training loss: 0.0843
[3512/15000], training loss: 0.0620
[3520/15000], training loss: 0.0666
16
AVD_Home_010_1_traj8, ate: 208.51079410724836
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[3528/15000], training loss: 0.0688
[3536/15000], training loss: 0.0702
[3544/15000], training loss: 0.0855
[3552/15000], training loss: 0.0744
[3560/15000], training loss: 0.0809
16
AVD_Home_010_1_traj8, ate: 216.8546867839308
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[3568/15000], training loss: 0.0684
[3576/15000], training loss: 0.0736
[3584/15000], training loss: 0.0726
[3592/15000], training loss: 0.0740
[3600/15000], training loss: 0.0782
16
AVD_Home_010_1_traj8, ate: 205.34880362783863
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[3608/15000], training loss: 0.0761
[3616/15000], training loss: 0.0685
[3624/15000], training loss: 0.0780
[3632/15000], training loss: 0.0769
[3640/15000], training loss: 0.0826
16
AVD_Home_010_1_traj8, ate: 210.21716366917343
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[3648/15000], training loss: 0.0779
[3656/15000], training loss: 0.0980
[3664/15000], training loss: 0.0750
[3672/15000], training loss: 0.0697
[3680/15000], training loss: 0.0782
16
AVD_Home_010_1_traj8, ate: 212.65933240932665
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[3688/15000], training loss: 0.0612
[3696/15000], training loss: 0.0767
[3704/15000], training loss: 0.0817
[3712/15000], training loss: 0.0824
[3720/15000], training loss: 0.0722
16
AVD_Home_010_1_traj8, ate: 212.00822850304402
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[3728/15000], training loss: 0.0709
[3736/15000], training loss: 0.0663
[3744/15000], training loss: 0.0711
[3752/15000], training loss: 0.0616
[3760/15000], training loss: 0.0690
16
AVD_Home_010_1_traj8, ate: 209.5732875012551
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[3768/15000], training loss: 0.0626
[3776/15000], training loss: 0.0583
[3784/15000], training loss: 0.0706
[3792/15000], training loss: 0.0648
[3800/15000], training loss: 0.0655
16
AVD_Home_010_1_traj8, ate: 210.02442410354547
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[3808/15000], training loss: 0.0709
[3816/15000], training loss: 0.0752
[3824/15000], training loss: 0.0773
[3832/15000], training loss: 0.0608
[3840/15000], training loss: 0.0881
16
AVD_Home_010_1_traj8, ate: 209.00384638750194
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[3848/15000], training loss: 0.0762
[3856/15000], training loss: 0.0796
[3864/15000], training loss: 0.0648
[3872/15000], training loss: 0.0933
[3880/15000], training loss: 0.0841
16
AVD_Home_010_1_traj8, ate: 209.4940226007977
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[3888/15000], training loss: 0.0658
[3896/15000], training loss: 0.0670
[3904/15000], training loss: 0.0836
[3912/15000], training loss: 0.0669
[3920/15000], training loss: 0.0914
16
AVD_Home_010_1_traj8, ate: 214.65421429038008
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[3928/15000], training loss: 0.0847
[3936/15000], training loss: 0.0822
[3944/15000], training loss: 0.1008
[3952/15000], training loss: 0.0707
[3960/15000], training loss: 0.0628
16
AVD_Home_010_1_traj8, ate: 208.08027046853488
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[3968/15000], training loss: 0.0803
[3976/15000], training loss: 0.0998
[3984/15000], training loss: 0.0743
[3992/15000], training loss: 0.0627
[4000/15000], training loss: 0.0758
16
AVD_Home_010_1_traj8, ate: 205.50165305722982
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[4008/15000], training loss: 0.0626
[4016/15000], training loss: 0.0692
[4024/15000], training loss: 0.0772
[4032/15000], training loss: 0.0741
[4040/15000], training loss: 0.0766
16
AVD_Home_010_1_traj8, ate: 211.86811855461377
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[4048/15000], training loss: 0.0664
[4056/15000], training loss: 0.0795
[4064/15000], training loss: 0.0769
[4072/15000], training loss: 0.0788
[4080/15000], training loss: 0.0577
16
AVD_Home_010_1_traj8, ate: 210.28237653797578
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[4088/15000], training loss: 0.0713
[4096/15000], training loss: 0.0610
[4104/15000], training loss: 0.0788
[4112/15000], training loss: 0.0804
[4120/15000], training loss: 0.1024
16
AVD_Home_010_1_traj8, ate: 210.24825863880136
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[4128/15000], training loss: 0.0633
[4136/15000], training loss: 0.0690
[4144/15000], training loss: 0.0746
[4152/15000], training loss: 0.0908
[4160/15000], training loss: 0.0667
16
AVD_Home_010_1_traj8, ate: 206.77500230999834
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[4168/15000], training loss: 0.0628
[4176/15000], training loss: 0.0837
[4184/15000], training loss: 0.0724
[4192/15000], training loss: 0.0578
[4200/15000], training loss: 0.0716
16
AVD_Home_010_1_traj8, ate: 205.13548844002582
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[4208/15000], training loss: 0.0799
[4216/15000], training loss: 0.0645
[4224/15000], training loss: 0.0765
[4232/15000], training loss: 0.0804
[4240/15000], training loss: 0.0589
16
AVD_Home_010_1_traj8, ate: 202.70785997299782
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[4248/15000], training loss: 0.1455
[4256/15000], training loss: 0.0797
[4264/15000], training loss: 0.0859
[4272/15000], training loss: 0.0994
[4280/15000], training loss: 0.1150
16
AVD_Home_010_1_traj8, ate: 211.35939623168267
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[4288/15000], training loss: 0.0717
[4296/15000], training loss: 0.0821
[4304/15000], training loss: 0.0768
[4312/15000], training loss: 0.0866
[4320/15000], training loss: 0.0844
16
AVD_Home_010_1_traj8, ate: 206.43387061256524
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[4328/15000], training loss: 0.0688
[4336/15000], training loss: 0.0685
[4344/15000], training loss: 0.0664
[4352/15000], training loss: 0.0706
[4360/15000], training loss: 0.0806
16
AVD_Home_010_1_traj8, ate: 204.35576130239164
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[4368/15000], training loss: 0.0974
[4376/15000], training loss: 0.0684
[4384/15000], training loss: 0.0680
[4392/15000], training loss: 0.0653
[4400/15000], training loss: 0.0712
16
AVD_Home_010_1_traj8, ate: 205.39986917701876
model saved to ../results/AVD/AVD_Home_010_1_traj8/model_best.pth
[4408/15000], training loss: 0.0749
[4416/15000], training loss: 0.0701
./lstm_run_train_AVD.sh: line 24:   496 Terminated              python lstm_train_AVD.py -o $MODE -g $GPUID -y $LAT --name $NAME -d $DATA_DIR -t ${TRAJ}.txt -e $EPOCH -b $BS -l $LOSS -n $N --log_interval $LOG
./lstm_run_train_AVD.sh: line 25: /home/mmvc: Is a directory
